# -*- coding: utf-8 -*-
"""News_Headlines_Sarcasm_Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ujcru6KMyVBksUFoYeUKUjzxmHnTBK6X
"""

from google.colab import drive
drive.mount('/content/drive')

pip install transformers

import transformers
from transformers import RobertaTokenizer, TFRobertaModel
import pandas as pd
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import nltk
from sklearn.preprocessing import LabelBinarizer
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from bs4 import BeautifulSoup
import re, string, unicodedata
from keras.preprocessing import text, sequence
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split
from string import punctuation
import tensorflow as tf
import tqdm
from sklearn.model_selection import train_test_split

"""**Data Preprocessing**"""

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

df = pd.read_json('/content/drive/MyDrive/Colab Notebooks/data/Sarcasm_Headlines_Dataset.json', lines=True)
df.drop('article_link', inplace=True, axis=1)
df.head()

stopwords = nltk.corpus.stopwords.words('english')  #later used this variable for removing stopwords from the json file

def strip_html_tags(text):
    soup = BeautifulSoup(text, "html.parser")
    [s.extract() for s in soup(['iframe', 'script'])]
    stripped_text = soup.get_text()
    stripped_text = re.sub(r'[\r|\n|\r\n]+', '\n', stripped_text)
    return stripped_text

def remove_stopwords(sentence):
    final_text = []
    for i in sentence.split():
        if i.strip().lower() not in stopwords:
            final_text.append(i.strip())
    return " ".join(final_text)

def clean_text(sentence):
    sentence = strip_html_tags(sentence)
    text = remove_stopwords(sentence)
    return sentence

df['headline'] = df['headline'].apply(clean_text)
X = df.drop(columns=['is_sarcastic'])
y = df['is_sarcastic']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train.shape, X_test.shape

train_clean_text = X_train['headline']
test_clean_text = X_test['headline']
train_text = X_train['headline']
test_text = X_test['headline']

"""**ROBERTA MODEL**"""

def create_roberta_input_features(tokenizer, docs, max_seq_length):
    all_ids, all_masks = [], []
    for doc in tqdm.tqdm(docs, desc="Converting docs to features"):
        tokens = tokenizer.tokenize(doc)
        if len(tokens) > max_seq_length - 2:
            tokens = tokens[0: (max_seq_length - 2)]
        tokens = ['<s>'] + tokens + ['</s>']
        ids = tokenizer.convert_tokens_to_ids(tokens)
        masks = [1] * len(ids)

        while len(ids) < max_seq_length:
            ids.append(1)  # <pad> token id for RoBERTa
            masks.append(0)  # 0 for padding
        all_ids.append(ids)
        all_masks.append(masks)
    encoded = np.array([all_ids, all_masks])
    return encoded

MAX_SEQ_LENGTH = 18
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

inp_id = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype='int32', name="roberta_input_ids")
inp_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype='int32', name="roberta_input_masks")
inputs = [inp_id, inp_mask]

hidden_state = TFRobertaModel.from_pretrained('roberta-base')(inputs)[0]
pooled_output = hidden_state[:, 0, :]  # Take the [CLS] token's output
dense1 = tf.keras.layers.Dense(256, activation='relu')(pooled_output)
drop1 = tf.keras.layers.Dropout(0.25)(dense1)
dense2 = tf.keras.layers.Dense(256, activation='relu')(drop1)
drop2 = tf.keras.layers.Dropout(0.25)(dense2)
output = tf.keras.layers.Dense(1, activation='sigmoid')(drop2)

model = tf.keras.Model(inputs=inputs, outputs=output)
model.compile(optimizer=tf.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08), loss='binary_crossentropy', metrics=['accuracy'])

model.summary()

train_features_ids, train_features_masks = create_roberta_input_features(tokenizer, train_clean_text, max_seq_length=MAX_SEQ_LENGTH)

inputs  = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=3, restore_best_weights=True, verbose=1)

yash_model_new = model.fit([train_features_ids, train_features_masks], y_train,validation_split=0.1,  epochs=10, batch_size=25, callbacks=[inputs], shuffle=True,verbose=1)

my_trained_model = model.save('/content/new_yash_model/yash_trained_model.h5')

test_features_ids, test_features_masks = create_roberta_input_features(tokenizer, test_clean_text, max_seq_length=MAX_SEQ_LENGTH)

predictions = [1 if pr > 0.5 else 0 for pr in model.predict([test_features_ids, test_features_masks], verbose=0).ravel()]

print(classification_report(y_test, predictions))
pd.DataFrame(confusion_matrix(y_test, predictions))

test_features_ids, test_features_masks = create_roberta_input_features(tokenizer, test_text, max_seq_length=MAX_SEQ_LENGTH)

predictions = [1 if pr > 0.5 else 0 for pr in model.predict([test_features_ids, test_features_masks], verbose=0).ravel()]

print(classification_report(y_test, predictions))
pd.DataFrame(confusion_matrix(y_test, predictions))

import matplotlib.pyplot as plt
import seaborn as sns

data = {'Class': df['is_sarcastic'].map({0: 'Non-Sarcastic', 1: 'Sarcastic'})}
plt.figure(figsize=(8, 6))
sns.countplot(x='Class', data=data)
plt.title('Class Distribution')
plt.xlabel('Class')
plt.ylabel('Count')
plt.show()

#train vs validation accuracy graph plot

# Plot training & validation accuracy values
plt.figure(figsize=(10, 6))
plt.plot(yash_model_new.history['accuracy'], label='Training Accuracy')
plt.plot(yash_model_new.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training vs. Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()



"""**Implementing CNN**"""

import pandas as pd
import numpy as np
import nltk
from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

"""Loading again to avoid variable conflicts"""

sarcasm_data = pd.read_json('/content/drive/MyDrive/Colab Notebooks/data/Sarcasm_Headlines_Dataset.json', lines=True)
sarcasm_data.drop('article_link', inplace=True, axis=1)

X = sarcasm_data['headline']
y = sarcasm_data['is_sarcastic']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Tokenization
max_words = 10000  # Maximum number of words in the vocabulary
max_sequence_length = 50  # Maximum length of input sequences

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(X_train)
X_train_sequences = tokenizer.texts_to_sequences(X_train)
X_test_sequences = tokenizer.texts_to_sequences(X_test)

X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length)
X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length)

"""**My CNN Model**"""

#CNN model
model2 = Sequential()
model2.add(Embedding(input_dim=max_words, output_dim=100, input_length=max_sequence_length))
model2.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
model2.add(MaxPooling1D(pool_size=5))
model2.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
model2.add(MaxPooling1D(pool_size=5))
model2.add(Flatten())
model2.add(Dense(128, activation='relu'))
model2.add(Dropout(0.5))
model2.add(Dense(1, activation='sigmoid'))

model2.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
model2.summary()

# Train the CNN model
epochs = 10
batch_size = 64

yk_cnn_model = model2.fit(X_train_padded, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1, verbose=1, callbacks=[early_stopping])

cnn_trained_model = model2.save('/content/new_yash_model/sarcasm_detection_cnn_model.h5')

# Evaluate the CNN model on the test data
y_pred = (model2.predict(X_test_padded) > 0.5).astype(int)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

class_mapping = {0: 'Non-Sarcastic', 1: 'Sarcastic'}
sarcasm_data['Class'] = sarcasm_data['is_sarcastic'].map(class_mapping)

plt.figure(figsize=(10, 6))
plt.plot(yk_cnn_model.history['accuracy'], label='Training Accuracy')
plt.plot(yk_cnn_model.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training vs. Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

roberta_cm = confusion_matrix(y_test, predictions)

# Plot confusion matrix for RoBERTa model
plt.figure(figsize=(8, 6))
plt.imshow(roberta_cm, interpolation='nearest', cmap='Blues')
plt.title('RoBERTa Model Confusion Matrix')
plt.colorbar()
tick_marks = np.arange(2)
plt.xticks(tick_marks, ["Non-Sarcastic", "Sarcastic"])
plt.yticks(tick_marks, ["Non-Sarcastic", "Sarcastic"])
plt.xlabel('Predicted')
plt.ylabel('Actual')
for i in range(2):
    for j in range(2):
        plt.text(j, i, str(roberta_cm[i, j]), horizontalalignment='center', color='black')
plt.show()

print("RoBERTa Model Confusion Matrix:")
print(np.array2string(roberta_cm, separator=', ', formatter={'int': lambda x: f"[{x}]"}))

# Plot confusion matrix for CNN model
plt.figure(figsize=(8, 6))
plt.imshow(cnn_cm, interpolation='nearest', cmap='Blues')
plt.title('CNN Model Confusion Matrix')
plt.colorbar()
tick_marks = np.arange(2)
plt.xticks(tick_marks, ["Non-Sarcastic", "Sarcastic"])
plt.yticks(tick_marks, ["Non-Sarcastic", "Sarcastic"])
plt.xlabel('Predicted')
plt.ylabel('Actual')
for i in range(2):
    for j in range(2):
        plt.text(j, i, str(cnn_cm[i, j]), horizontalalignment='center', color='black')
plt.show()
print("\nCNN Model Confusion Matrix:")
print(np.array2string(cnn_cm, separator=', ', formatter={'int': lambda x: f"[{x}]"}))

"""# ***Inferences***"""

print("Inference:")
print("For the RoBERTa Model:")
print(" - High accuracy (0.92) indicates that it performs well overall.")
print(" - Precision and recall for class 0 are high (0.90 and 0.97), indicating good detection of non-sarcastic headlines.")
print(" - Precision and recall for class 1 are also good (0.95 and 0.87), indicating good detection of sarcastic headlines.")
print(" - F1-scores are high for both classes, indicating a good balance between precision and recall.")

print("\nFor the CNN Model:")
print(" - Accuracy (0.85) is slightly lower than RoBERTa, but still reasonable.")
print(" - Precision and recall for class 0 are decent (0.87 and 0.87), indicating reasonable detection of non-sarcastic headlines.")
print(" - Precision and recall for class 1 are also decent (0.83 and 0.84), indicating reasonable detection of sarcastic headlines.")
print(" - F1-scores are also decent for both classes.")

"""# For the RoBERTa Model:
 - High accuracy (0.92) indicates that it performs well overall.
 - Precision and recall for class 0 are high (0.90 and 0.97), indicating good detection of non-sarcastic headlines.
 - Precision and recall for class 1 are also good (0.95 and 0.87), indicating good detection of sarcastic headlines.
 - F1-scores are high for both classes, indicating a good balance between precision and recall.

## For the CNN Model:
 - Accuracy (0.85) is slightly lower than RoBERTa, but still reasonable.
 - Precision and recall for class 0 are decent (0.87 and 0.87), indicating reasonable detection of non-sarcastic headlines.
 - Precision and recall for class 1 are also decent (0.83 and 0.84), indicating reasonable detection of sarcastic headlines.
 - F1-scores are also decent for both classes.
"""

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Compute ROC curve and ROC area for RoBERTa model
roberta_fpr, roberta_tpr, _ = roc_curve(y_test, model.predict([test_features_ids, test_features_masks]))
roberta_roc_auc = auc(roberta_fpr, roberta_tpr)

# Compute ROC curve and ROC area for CNN model
cnn_fpr, cnn_tpr, _ = roc_curve(y_test, y_pred)
cnn_roc_auc = auc(cnn_fpr, cnn_tpr)

# Plot ROC curves
plt.figure(figsize=(10, 6))
plt.plot(roberta_fpr, roberta_tpr, color='darkorange', lw=2, label='RoBERTa (AUC = %0.2f)' % roberta_roc_auc)
plt.plot(cnn_fpr, cnn_tpr, color='blue', lw=2, label='CNN (AUC = %0.2f)' % cnn_roc_auc)
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.show()

"""**ROC Curves**

AUC value represents the area under the ROC curve, which provides a single metric for comparing the models' overall performance. A higher AUC indicates better discrimination between positive and negative classes.

# RoBERTa Model (AUC = 0.98):

The RoBERTa model exhibits excellent discrimination between positive and negative classes.
An AUC value of 0.98 indicates that the model has a high true positive rate (Sensitivity) while maintaining a low false positive rate (1 - Specificity).
It is very effective at distinguishing between sarcastic and non-sarcastic headlines.

# CNN Model (AUC = 0.85):

The CNN model also demonstrates reasonable discrimination between positive and negative classes.
An AUC value of 0.85 suggests that the model is reasonably effective at classifying the data, but it may have some room for improvement.
While the CNN model performs well, it doesn't perform as well as the RoBERTa model in terms of distinguishing between classes

# **Outcomes**

The higher AUC for the RoBERTa model suggests that it outperforms the CNN model in terms of discrimination and overall classification performance. I find this indication quite compelling.

Although the CNN model has an AUC of 0.85, I believe it still provides reasonably good results and can be a valid choice for this task.

To enhance the CNN model's performance, I can consider the following strategies:


1.   I can explore hyperparameter tuning, which involves adjusting parameters like the learning rate, batch size, and configurations of convolutional layers. This approach may help fine-tune the model's performance.
2.   Experimenting with different CNN architectures or exploring more complex models is an option I can explore. This allows me to assess if a different model structure might yield better results.
3. If I have a limited dataset, I can implement data augmentation techniques. This involves creating variations of the existing data to increase the size of the training dataset. It can help the model generalize better.
4. Lastly, I can delve into feature engineering and text preprocessing techniques. This might include extracting additional features from the text or applying specific text cleaning and transformation methods to enhance the model's performance.
"""

